# -*- coding: utf-8 -*-
"""DeekSeek fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f5vbYov3gI8l3CcP2kAUh51BBgVlXNiW
"""

!pip install -q transformers datasets peft accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType
import torch

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.float16).to("cuda")

model

from peft import LoraConfig, TaskType

lora_config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

from datasets import load_dataset

dataset = load_dataset("Malikeh1375/medical-question-answering-datasets", name="all-processed", split="train")

print(dataset[0])

print(len(dataset))

import json

formatted_data = []

for item in dataset:
    prompt = f"{item['instruction']}\n\n{item['input']}"
    output = item['output']
    formatted_data.append({
        "prompt": prompt,
        "output": output
    })

print(formatted_data[0])

def tokenize(sample):
    full_prompt = sample["prompt"]
    full_output = sample["output"]

    full_text = full_prompt + "\n\n" + full_output

    tokenized = tokenizer(
        full_text,
        truncation=True,
        max_length=512,
        # max_length=1024,
        padding="max_length",
        return_tensors=None
    )

    tokenized["labels"] = tokenized["input_ids"].copy()

    return tokenized

from datasets import Dataset

train_dataset = Dataset.from_list(formatted_data)

tokenized_dataset = train_dataset.map(tokenize, remove_columns=["prompt", "output"])

import zipfile

zip_path = "/mnt/data/tokenized_medical_dataset.zip"
extract_path = "/content/tokenized_medical_dataset"

with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_path)

# tokenized_dataset.save_to_disk("tokenized_medical_dataset")

# import shutil
# shutil.make_archive("tokenized_medical_dataset", 'zip', "tokenized_medical_dataset")

from datasets import load_from_disk
tokenized_dataset = load_from_disk("tokenized_medical_dataset")

print(len(tokenized_dataset))

subset_dataset = tokenized_dataset.shuffle(seed=42).select(range(30000))

print(len(subset_dataset))

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./deepseek_lora_ckpt",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=5e-4,
    num_train_epochs=5,
    save_steps=4000,
    logging_steps=400,
    save_total_limit=2,
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=subset_dataset,
    tokenizer=tokenizer,
)

trainer.train()

trainer.save_model("./final_deepseek_model")

import shutil

shutil.make_archive("final_deepseek_model", "zip", "final_deepseek_model")