# -*- coding: utf-8 -*-
"""Copy of checking model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SORgz-9Hmb5cR7VVPVyms-1AbtY671T4
"""


import os
import time
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
from tqdm import tqdm
from openai import OpenAI

client = OpenAI(
    base_url="https://api.openai.com/v1",
    api_key= os.environ['OPENAI_API_KEY'],
)

from huggingface_hub import notebook_login

notebook_login()

import os
import json
import time
import pandas as pd
import torch
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
from tqdm import tqdm


BASE_URL = "https://api.openai.com/v1"
OPENAI_KEY_ENV = "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"
DEFAULT_NUM_EXAMPLES = 30  

MODEL_CONFIGS = [
    {
        "name": "deepSeek-R1",
        "base_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "lora_dir": "./drive/MyDrive/Research/cs7650/final_deepseek_model"
    },
    {
        "name": "llama",
        "base_model_id": "meta-llama/Llama-3.2-3B-Instruct",
        "lora_dir": "./drive/MyDrive/Research/cs7650/llama-medical-lora"
    },
    {
        "name": "SmolLM2",
        "base_model_id": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
        "lora_dir": "./drive/MyDrive/Research/cs7650/final_smol_model"
    }
]

def setup_openai_client(api_key_env=OPENAI_KEY_ENV, base_url=BASE_URL):
    return OpenAI(base_url=base_url, api_key=api_key_env)

FUNCTIONS = [{
    "name": "generate_questions",
    "description": "Generate patient-style medical questions.",
    "parameters": {
        "type": "object",
        "properties": {
            "questions": {
                "type": "array",
                "minItems": DEFAULT_NUM_EXAMPLES,
                "maxItems": DEFAULT_NUM_EXAMPLES,
                "items": {"type": "object", "properties": {"question": {"type": "string"}}, "required": ["question"]}
            }
        },
        "required": ["questions"]
    }
}]

def generate_questions(client, prompt_text):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a patient asking about symptoms, causes, and treatments."},
            {"role": "user", "content": prompt_text}
        ],
        functions=FUNCTIONS,
        function_call={"name": "generate_questions"},
        temperature=0.7
    )
    args = json.loads(response.choices[0].message.function_call.arguments)
    return [q["question"] for q in args.get("questions", [])]

def evaluate_interpretability_vote(client, prompt, response_a, response_b):
    system_msg = "You are a layperson evaluator assessing clarity from a patient's perspective."
    user_msg = f"""
You are asked to compare two answers to the following patient-style question for interpretability:

Question:
{prompt}

Answer A:
{response_a}

Answer B:
{response_b}

Reply with only a JSON object:
{{
  "interpretability_vote": "<A or B or Tie>",
  "interpretability_reason": "<brief explanation>"
}}"""
    comp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
        temperature=0.0
    )
    return json.loads(comp.choices[0].message.content)
def load_tokenizer_and_model(model_id, device_map="auto", torch_dtype=torch.float16, lora_dir=None):
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, torch_dtype=torch_dtype, trust_remote_code=True)
    if lora_dir:
        peft_cfg = PeftConfig.from_pretrained(lora_dir)
        model = PeftModel.from_pretrained(model, lora_dir, device_map=device_map, torch_dtype=torch_dtype)
    model.eval()
    return tokenizer, model


def get_model_answer(tokenizer, model, device, question):
    prompt = f"Question: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad(): outputs = model.generate(**inputs, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text.split('Answer:')[-1].strip()


def main():
    client = setup_openai_client()
    patient_prompt = (
        f"Generate exactly 35 patient-style questions phrased as if you are a patient, "
        "asking about your symptoms, possible causes, and treatment options. Describe symptoms in two sentences. "
        "One example is: 'Iâ€™ve noticed some changes in my vision recently. Over the past couple of days, "
        "my eyes have felt dry and sensitive to light. Should I be concerned? What should I do about it?'"
    )

    patient_questions = generate_questions(client, patient_prompt)
    print(f"Generated {len(patient_questions)} patient_style questions")
    if len(patient_questions) > DEFAULT_NUM_EXAMPLES:
        patient_questions = patient_questions[:DEFAULT_NUM_EXAMPLES]
        print(f"Trimmed to first {DEFAULT_NUM_EXAMPLES} questions")

    pd.DataFrame(patient_questions, columns=['question']).to_csv('questions_patient_style.csv', index=False)
    print('Saved patient-style questions to CSV')

    for config in MODEL_CONFIGS:
        name = config["name"]
        tokenizer_base, model_base = load_tokenizer_and_model(config["base_model_id"])
        tokenizer_ft, model_ft = load_tokenizer_and_model(config["base_model_id"], lora_dir=config["lora_dir"])
        device = next(model_base.parameters()).device
        records = []

        for q in tqdm(patient_questions, desc=f"{name}-patient_style"):
            ans_base = get_model_answer(tokenizer_base, model_base, device, q)
            ans_fine = get_model_answer(tokenizer_ft, model_ft, device, q)
            comp_iv = evaluate_interpretability_vote(client, q, ans_base, ans_fine)
            records.append({
                "model": name,
                "question": q,
                "base_answer": ans_base,
                "fine_tuned_answer": ans_fine,
                "interpretability_vote": comp_iv.get("interpretability_vote"),
                "interpretability_reason": comp_iv.get("interpretability_reason")
            })
            time.sleep(1)

        df = pd.DataFrame(records)
        df.to_csv(f'interpretability_{name}.csv', index=False)

        iv_counts = df['interpretability_vote'].value_counts().to_dict()
        summary = [{
            'model': name,
            'interpretability_A': iv_counts.get('A', 0),
            'interpretability_B': iv_counts.get('B', 0),
            'interpretability_Tie': iv_counts.get('Tie', 0)
        }]
        pd.DataFrame(summary).to_csv(f'summary_{name}.csv', index=False)

        del model_base, model_ft, tokenizer_base, tokenizer_ft
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()





