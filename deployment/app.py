from flask import Flask, request, render_template, jsonify
import os, requests
from checking_model import output_verifier
from openai import OpenAI

app = Flask(__name__)

LLM1_URL = "http://localhost:8000/v1/chat/completions"
LLM2_URL = "http://localhost:8080/v1/chat/completions"
client = OpenAI(
    base_url="https://api.openai.com/v1",
    api_key= "sk-lQa8WYxoPgfFxoPs-Oqf6_VDr4jcVcQbKYbZDl8N-JT3BlbkFJL8-B3Ivu0QkSjWSrlLTU5_undnDZsEtrMGVVUEtO8A"
)
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/medical', methods=['POST'])
def medical():
    data = request.get_json(force=True)
    prompt = data.get('input', '')
    payload = {
    "model": "./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/",
    "messages": [
        {"role": "system", "content": "You are a doctor and you need to answer the following question."},
        {"role": "user", "content": prompt},
    ],
    "stream": False,
    "max_tokens": 1000,
    }
    consistent = False
    max_attempts = 3
    cnt = 0
    while not consistent and cnt < max_attempts:
        cnt += 1
        r1 = requests.post(LLM1_URL, json=payload)
        agent1_response = r1.json()['choices'][0]['message']['content']
        
        r2 = requests.post(LLM2_URL, json=payload)
        agent2_response = r2.json()['choices'][0]['message']['content']
        
        verified_output = output_verifier(prompt, agent1_response, agent2_response)
        print("verified_output")
        print(verified_output)
        
        if verified_output['consistent'] == "Yes":
            consistent = True
            break

    if consistent == True:
        output_verification_output = "The responses are consistent."
        final_output = len(agent1_response) > len(agent2_response) and agent1_response or agent2_response
    else:
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a medical expert evaluator."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )
        final_output = completion.choices[0].message.content.strip()
        output_verification_output = "The responses are inconsistent. The final output is generated by the Verification model."

    return jsonify({
        'agent1':  agent1_response,
        'agent2':  agent2_response,
        'verified': output_verification_output,        
        'final':   final_output
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
